window.searchIndex = [{"url":"https://f4z3r.github.io/","title":"","body":""},{"url":"https://f4z3r.github.io/beyond-the-pod-webassembly-and-wasmcloud-next-platform-evolution/","title":"Beyond the Pod: Why wasmCloud and WebAssembly Might Be the Next Evolution of the Platform","body":"Over the past few months I have invested some time to contribute to an open source project I find\nfascinating: wasmCloud. As a platform engineer and architect, I am very\nfamiliar with how software platforms are typically built in practice. However, with the ubiquity of\nKubernetes, you run the risk to being stuck in the \"doing it the Kubernetes way\" line of thinking.\nBut then again, are there any better ways? This is where wasmCloud caught my attention. A modern\nplatform building on proven concepts from Kubernetes, but with some significant differences. In this\narticle I want to introduce wasmCloud, how it compares to Kubernetes, what its internal architecture\nlooks like, and what ideas are, in my humble opinion, a step up from \"the Kubernetes way of things\".\nBefore getting started, I need to get some things out of the way. This article will make quite a few\ncomparisons to Kubernetes and bytecode interpreters like the JVM. If you are unfamiliar with these\ntechnologies, it might make sense to have a short look at what these are. Considering you clicked on\nthis article, I am however guessing that you are familiar with them and have some experience in\nplatform engineering practices, either as a poweruser of a platform, or as a designer and developer\nof one.\nMoreover, I want to thank the company I work for, ipt, for allowing me to\ninvest time to learn about new technologies such as wasmCloud. Not only is contributing to open\nsource a great way to pay back a community powering the modern world, it is also a huge passion of\nmine. Being able to help the development of such projects during paid worktime enables me to learn\nso much on emerging technologies, and maybe help build the revolutionary tools of tomorrow.\nSo... wasmCloud!? I have been interested in WebAssembly ever since it promised to replace\nJavaScript, a language I personally consider as extremely poorly designed (someone once told me it\nwas designed in three days, so no wonder there). While WebAssembly is very far from doing anything\nclose to replacing JavaScript in the browser, it has evolved into something else: an application\nruntime and a potential replacement for containers.\nWebAssembly as a Platform Foundation\nModern platforms nearly all build on top of containers as their foundational element to run\nexecutable code. This is a logical evolution from Docker's meteoric growth, and the ecosystem that\ngrew around its open standards (such as the\nOCI - Open Container Initiative). While containers provide a huge\nstep in terms of ease of use, standardization, and security compared to shipping raw artefacts to\nvirtual machines, as was the case before them, they do have some shortcomings.\nFirst and foremost, containers are not composable. In part due to their flexibility, they do not\noffer standard ways of expressing how the world should interact with them at runtime, or what they\nrely on to perform their functionality. This means that containers are typically deployed as\nREST-based microservices, where containers communicate with one another over a network using APIs\nagreed upon outside of the container standards. This lack of standardization makes building reusable\ncomponents more challenging than it has to be. Moreover, each container essentially needs a server,\nauthentication, authorization, and more to run. This results in quite some waste in the compute\ndensity of the platform, with lots of compute wasted on boilerplate.\nMoreover, while containers are a huge step in the right direction in terms of security, they are not\nquite as secure as most people are led to believe. Containers are \"allow by default\" constructs,\nwhich take quite some work to properly harden.\nFinally, due to how containers are typically built, their startup times are not that great. It is\nnot abnormal to see container start times in the dozens of seconds. This does not bother people very\nmuch because containers are mostly used to run long running processes (since we need these REST APIs\neverywhere). However, a large part of containers are mostly idle, waiting for some API request to\ncome in. If one considers that workloads could be called (and thus the process started) only when\nneeded, startup times over 100ms is considered slow.\nThis is where WebAssembly comes it. WebAssembly addresses these challenges. Composability is\naddressed by the component model.\nWebAssembly: The Component Model\nThe component model is a way that WebAssembly\nmodules can be built with metadata attached to them which describe their imports and exports based\non a rich type system. Moreover, they are composable such that a new component can be built from\nexisting components as long as the imports of one are satisfied by the exports of another. This\nmeans that components can interact with one another via direct method/function calls, whose\nspecification is fully standardized. This interface specification is declared in a language known as\nthe WebAssembly Interface Types (WIT) language. An example of a WIT specification of a component\nrelying on a system clock can be seen below:\nWIT can be compared to the\nInterface Definition Language (IDL)\nfrom gRPC but for wasm components.\nThis declaration says that the component relies on an interface wall-clock (it imports the\ninterface) which defines two functions: now and resolution. Both take no arguments and return a\ndatetime object consisting of a seconds and nanoseconds field. This component could then be\ncomposed with any other component which exports this wall-clock interface.\nIf this were a container which would rely on accessing some API, we would need to read a\nnon-standardized documentation of the container image, and then read up on other containers to\nensure they provide APIs that match the ones called by the first container.\nThe WebAssembly component model can essentially be seen as a form of contract-based programming to\nformalize interfaces between WebAssembly core modules.\nWebAssembly: Secure by Default\nWhereas containers provide some form of security by namespacing processes and filesystems,\nWebAssembly actually sandboxes modules such that they cannot affect one another, or the host they\nrun on. By default a WebAssembly module cannot perform any privileged action and needs to be granted\nexplicit permission. I will not dive deeper into the details of this or I might loose myself in a\nrant on how software security in the modern day and age is abysmal.\nWebAssembly: Performance\nWebAssembly's main goal is performance. This means that WebAssembly modules run fast, but also that\nloading modules and starting them is much faster than containers. This has proven to be very useful\nalready, for instance in use cases such as serverless computing, where hyperscalers heavily rely on\nWebAssembly as a runtime to reduce cold start times, and reduce the delay in function calls.\nConsidering the idea to avoid having long running servers providing REST APIs and move to raw\nfunction calls on short running modules, having extremely short start times is imperative.\nAlright, so we can see that WebAssembly can be a great choice for the foundation runtime of a\nplatform. So where are platforms leveraging this? Well, actually, quite some \"platforms\" leverage\nthis idea already. For instance, SpinKube does exactly this, enabling\nto run WebAssembly functions on Kubernetes. However, you still interact with these functions via a\nREST call. Another example is Kubewarden, leveraging WebAssembly\nmodules to evaluate policies. While some might argue that this is not a platform, Kubewarden\nprovides a runtime for arbitrary programs, including their scheduling and deployment. Sounds like a\nplatform to me.\nFinally: wasmCloud! wasmCloud is probably what people would consider the closest to a full blown\nplatform to run WebAssembly modules. In other words, what Kubernetes is to containers, wasmCloud is\nto WebAssembly components. It provides a way to deploy, schedule, link, and lifecycle WebAssembly\ncomponents on a distributed platform.\nwasmCloud Architecture\nLet us look at the wasmCloud architecture a little.\nThis section will contain quite a few comparisons to Kubernetes concepts.\nGenerally, the wasmCloud architecture can be seen as quite similar to the Kubernetes architecture,\nwith the difference being that wasmCloud does not provide as much flexibility in swapping out\nbuilding blocks as Kubernetes does. This makes sense as it is a more nascent technology and is\ncurrently more opinionated.\nAs a reference, here is the diagram wasmCloud uses to provide an overview of the platform:\nAn overview of the wasmCloud platform\nAs one can see, the architecture is essentially a set of hosts connected via a so called \"lattice\".\nThus, the architecture distributes the runtime over a set of compute instances in order to achieve\nresilience against hardware/compute failures. The principle is identical to the one from Kubernetes,\nproviding a cluster in order to be able to quickly shift payloads on the platform to different nodes\nin case of node failures.\nHosts\nwasmCloud hosts are the foundation of the compute platform that is provided. They are the equivalent\nof Kubernetes nodes and provide a WebAssembly runtime for components to run on. Just as with\nKubernetes nodes, application developers will rarely need to worry about the hosts other than for\ndeployment affinities and the like.\nIn practice, hosts can be anything from a virtual machine, an IoT device, or even a pod running on\nKubernetes. In fact, hosting wasmCloud on Kubernetes is a relatively straight forward way to get\nstarted with the technology, providing wasmCloud as an application runtime, while providing services\nvia Kubernetes.\nLattice\nThe wasmCloud lattice is its networking layer. This can seem a bit strange when considering that\nthis a NATS instance.\nFor those unfamiliar with NATS: it is an event streaming component similar to Kafka, but provides\nadditional features such as a key values store, an object store, and publish-subscribe\ncapabilities.\nHaving a NATS instance as the \"networking layer\" confused me quite a lot at first. However, one has\nto remember that thanks to the component model, we no longer require HTTP/TCP network calls for our\ncomponents to interact with one another. Thus we don't necessarily need an IP to address a component\nwe want to reach. Of course NATS itself will require a physical network to run on in order to\ndistribute events to its different instances, but wasmCloud then only needs to use NATS.\nEssentially, every component exposing a function becomes a subscriber to a queue for this function\non NATS. Other components can then call this function via wRPC (gRPC for WebAssembly) by publishing\na call to some subject. This is quite different from Kubernetes networking, where calls need to know\nthe location of the callee in the network. Using a subject-based addressing model simplifies\ndeployment and improves scaling and resilience.\nAs a user of wasmCloud, you do not need to worry about this though. How function calls are preformed\nunder the hood is abstracted away from the user.\nThis distributed networking aspect is one of the superpowers of wasmCloud, as one does not need to\nworry about how to address a component on the platform. However, it can also introduce strange\nbehaviour in some cases. For instance, on Kubernetes, it's common sense that a HTTP call to a\ndifferent pod running on the cluster can fail. On wasmCloud however, if the interface we are calling\nfrom a different component returns some type, we use the component like a raw function call in our\ncomponents code. What if that call fails, not because of the called component but due to a\nnetworking issue? In the current implementation of wasmCloud this will lead to a panic in the\ncaller. As this is typically not the desired outcome, efforts are underway to design an adapted way\nhow the interfaces need to be designed to handle failures in the transport layer. On top of that,\nfunction calls might change such that might avoid using NATS as a transport layer if the component\nbeing called in on the same host and the caller.\nCapabilities\nThis is where Kubernetes and wasmCloud start differing in their philosophy. Thanks to the\nstandardized way interfaces can be declared in the component model, one can describe an abstract\ninterface which provides some functionality, without providing an implementation. This is what\ncapabilities are. They are abstract interfaces that describe some useful functionality, such as\nreading and writing to a key value store, or retrieving some sensitive information from a secured\nenvironment. These capabilities are published on wasmCloud for applications to use.\nAn application developer can then write a component that makes use of that interface if he/she needs\nthat functionality. He/she does not need to worry about how this capability is implemented. He\nrelies on the \"contract\" provided by the capability.\nIn my opinion, while this is quite challenging to grasp initially, this is what makes wasmCloud so\npromising. Having worked on many platforms in the past, the main challenge is always how additional\nservices can be provided on top of raw platforms such as Kubernetes in a way that makes then highly\nstandardized while easily consumable. In the current state of platform engineering, this quickly\nbecomes a question of good product management. Unfortunately, doing this correctly is surprisingly\ndifficult. Capabilities provide a technical solution to this, with the only limitation being\ncomplete incompatibility with existing software.\nProviders\nA provider is a specific implementation of a capability. For instance, taking the example of the\ncapability enabling the reading and writing to a key value store, a provider might implement this by\nhaving a ValKey instance backing the capability. Another provider might\nimplement the very same capability using NATS, Redis, or even an in-memory key-value store.\nAbstracting the provider away from the consumer via a capability enables the platform to swap\nproviders based on needs. Of course performing such a swap might be quite complex, for instance\ninvolving a data migration from NATS to ValKey. However, the beauty is that the applications do not\nrequire any changes as would be the case in traditional platforms.\nIt should be noted that the provider might run completely outside of wasmCloud itself. However,\nwasmCloud also provides internal providers that are backed into the hosts themselves, providing\nfunctionality such as logging or randomness.\nComponents\nComponents refer to the WebAssembly payload that contain your business logic. In the traditional\nsense, this is your application. However, in wasmCloud lingo, an application is a set of interlinked\ncomponents including all information about what capabilities they require.\nApplications\nApplications are an abstraction enabling to declaratively define a combination of components,\ncapabilities, and providers together into a deployable unit. Applications are based on the\nopen application model (OAM) and should thus look quite familiar to people\nworking with Kubernetes. In terms of definition, they are similar to a Kubernetes Deployment,\ndescribing not only the deployment unit (component or pod in the Kubernetes context), but also its\nreplication, affinities, links to capabilities, etc.\nIt should be noted that in wasmCloud v2, applications are re-worked to be much more closely\nmodelled after Kubernetes Deployments and ReplicaSets. Version 2 drops the idea of Applications\nalltogether and uses Workload, WorkloadReplicaSets, and WorkloadDeployments objects. These\nare also no longer linked to the OAM. In all likelihood we will write another blog post showcasing\nthe capabilities of composition provided by version 2 in the future.\nwadm\nThe wasmCloud Application Deployment Manager (wadm) manages Applications. It can be seen as the\ndeployment controller from Kubernetes for wasmCloud Applications. It essentially orchestrates the\ndeployment of components, capabilities, their links, etc. on the platform. This construct will also\nbe dropped with wasmCloud version 2.\nVerdict\nWith a decent understanding of the architecture we can now get an idea of the uses of wasmCloud in\nthe real world. While I have not yet run anything productive on wasmCloud, I have played with the\nplatform a lot over the past few months, and have come to really appreciate some of its innovative\nideas.\nThus, to summarise my experience: wasmCloud is a relatively new platform and provides interesting\nnew approaches to how inter-component communication can be modeled. On top of that, it does it while\nbuilding on open standards such as WebAssembly and the component model, such that the business logic\nof your application remains portable. While these new concepts are very promising, wasmCloud still\nsuffers from a couple drawbacks:\nFor people unfamiliar with WebAssembly, it has a quite steep learning curve. This is highly\naccentuated for people unfamiliar with existing platforms such as Kubernetes.\nThe set of supported providers and capabilities is extremely small to date. This will of course\ngrow as adoption increases, but currently early adopters will have to write their own providers\nmost of the time and will not be able to rely on third-party components.\nAs wasmCloud shifts more responsibility to the platform level, it will require a strong platform\nteam to operate this with low developer friction. This can be an issue as finding highly skilled\nplatform engineers is quite difficult at the moment. However, the team behind wasmCloud is focused\non making application delivery as frictionless as possible.\nFinally, I am not sure I currently understand the security model wasmCloud uses to authenticate\nand authorize calls between components. While I am not sure this is a drawback, it does not yet\nfeel as intuitive as Kubernetes simple yet relatively powerful RBAC. I will have to dive deeper\ninto this to have a final opinion on it though (another blog post might follow on this)."},{"url":"https://f4z3r.github.io/dissecting-kubewarden-internals-built-comparison/","title":"Dissecting Kubewarden: Internals, How It's Built, and Its Place Among Policy Engines","body":"Kubernetes offers amazing capabilities to improve compute density compared to older runtimes such as\nvirtual machines. However, in oder to leverage the capabilities of the platform, these tend to host\napplications from various tenants. This introduces a strong need for properly crafted controls and\nwell-defined compliance to ensure the tenants use the platform correctly and do not affect one\nanother. The RBAC capabilities provided out of the box by Kubernetes are quickly insufficient to\naddress this need. This is where policy engines such as Kubewarden\ncome into play. In this post we will look at how Kubewarden can be leveraged to ensure correct usage\nof a platform, how it compares to other policy engines, and how to best adopt it.\nPolicy Engines\nKubernetes provides role-based access control (RBAC) out of the box to control what actions can be\nperformed against the Kubernetes API. Generally, RBAC works by assigning sets of roles to users or\ngroups of users. Capabilities are attached to these roles, and users having a role obtain these\ncapabilities. This simple mechanism is very powerful, mostly because it is quite flexible while\nallowing a simple overview of a user's capabilities. However, in the case of Kubernetes, the\ndefinition of capabilities is very restricted. Roles only allow or deny access to Kuberenetes API\nendpoints, but to not allow control based on payload content. This means that these capabilities are\nmostly restricted to CRUD operations on Kubernetes primitives (e.g. Deployments, Ingresses, or\ncustom resources). Unfortunately, this is often not enough.\nFor instance, it is quite common to allow users to perform actions on some primitives under specific\nconditions. An example would be that creating Deploymentss is only allowed as long as its name\nfollows some convention and the pods its creates are not privileged and set proper resource\nrequests/limits. The naming convention cannot be enforced by standard RBAC controls as these have no\npossibility to represent more complex logic. Controlling the configuration of the pods created by a\nDeployment is a validation of the payload pushed to the API, and is thus not supported either.\nNote: Security contexts and resources on pods can be controlled via methods such as Security\nContext Constraints or Pod Security Policies and ResourceQuotas. However, these do not reject the\ncreation of the deployment, but will only block the creation of the pods themselves. It is\ntherefore possible to apply a Deployment that is known to not allow the creation of pods. In my\npersonal opinion this is not ideal, as it does not fail early.\nThese scenarios is where policy engines come into play. They utilise Kubernetes' Dynamic Access\nControl mechanisms to enable cluster administrators to manage permissions using more complex logic.\nThe exact capabilities of policy engines can vary greatly as these are essentially arbitrary\nsoftware that validates or mutates Kubernetes requests. However, the majority of major policy\nengines work similarly. They tend to implement the operator pattern, enabling the configuration of\npolicies using Kubernetes custom resources. In this blog post we will have a look at Kubewarden in\nmore detail, and how it compares to other engines.\nKubewarden Architecture\nKubewarden leverages WebAssembly (WASM) to enable extremely flexible\npolicy evaluation. Essentially, Kubewarden can be seen as a WASM module orchestrator where policies\nare deployed as serverless functions that get called when necessary. The result of these WASM\nfunctions then determines whether an API request against Kubernetes is allowed, denied, or altered\n(mutated).\nThis similee can also help explain Kubewarden's architecture. Essentially, the Kubewarden controller\n(operator) manages policy servers and admission policies. Policy servers can be seen as hosts for\nthe serverless execution of functions, whereas admissions policies are the functions themselves.\nTherefore, in order to perform policy validation, one needs at least one policy server running to\nhost the policies one wants to enforce. The controller then takes care of configuring the runtime\n(policy server) to properly run the adequate policy executable with the appropriate inputs when a\npolicy needs to be evaluated. The diagram below illustrates this:\nAs policies are WASM modules, they can themselves support configuration. This makes policy reuse a\nmajor feature of Kubewarden. Complex logic can be contained in the WASM module while exposing some\ntuning as configuration, allowing a policy to perform a relatively generic task. To understand this\nbetter, let us have a look at such a policy:\nIn this example, we are using a WASM module which evaluates a\nCommon Expression Language (CEL) expression to define our policy. Evaluating a\nCEL expression is not something we want to implement every time ourselves. Thankfully, Kubewarden\nprovides this as a WASM module on their\nArtefactHub. Thus we do not\nneed to implement anything and can reuse that module. It is referenced on line 7 above. Of course we\nalso need to actually define the CEL expression that should be the heart of the policy rule. This is\ndone on lines 18 to 25 within the settings block. Note how we can use object internals (such as\nreplicas defined in a Deployment) in the validation expression. Finally, we need to define on what\nobjects this policy should be evaluated. In order to do this, we provide rules (lines 13-16) that\ntell Kubewarden on what Kubernetes API endpoints to trigger the policy, and additionally provide\ninformation about which namespaces should be affected by the policy with a namespaceSelector\n(lines 26-28). The remaining options configure the following:\nbackgroundAudit: informs Kubewarden to report on this policy for objects that are already\ndeployed. In this case, we validate the replicas on created or updated Deployment objects.\nHowever, there might already be Deployments on the cluster that violate the policy before we\nstart enforcing it. This option will tell Kubewarden to provide reports on such violations.\nmode: Kubewarden supports enforcing policies (in protect mode), or monitoring the cluster (in\nmonitor mode). Using the monitor mode can be interesting when investigating how people use the\nKubernetes cluster or providing them with warnings before enforcing policies.\nmutating: policies can also mutate (change) requests. In this case we are only performing\nvalidation to potentially reject requests. Thus we set mutating to false.\npolicyServer: as explained above, Kubewarden can manage many policy servers. This simply informs\nthe controller on which policy server this specific policy should be deployed.\nAs one can see based on the sample policy above, while Kubewarden technically uses programs as\npolicies, it is usually not necessary to write any code to use Kubewarden. This is thanks to its\nstrong focus on module configuration and re-usability. The above CEL module alone already enables\nthe configuration of a very wide range of policies. On top of that, other modules shared on\nArtefactHub provide more specific validations or mutations that might incorporate more complex\nlogic. If this is not enough, policy groups (a feature we will not cover in this post) can be\nutilised to combine other policies and express more complex logic as well. Finally, if one has very\nspecific needs that cannot be addressed by any of the publicly shared modules, one can still fall\nback to writing code and building ones own module with fully arbitrary logic. How such policies can\nbe written, in actual code, might follow in a separate blog post.\nThe above architecture of Kubewarden is what makes it stand apart from most other policy engines.\nGenerally policy engines contain the logic fully in the controller, only exposing configuration via\nthe custom resource. Since Kubewarden can essentially execute arbitrary WASM bytecode, it is not\nbound by the expressiveness of the custom resource declaration.\nAll this considered, is Kubewarden the best choice for a policy engine and should be used in all\nscenarios?\nComparison\nThere are many other policy engines out there, such as Kyverno,\nGatekeeper, or\nPolaris. So why would you choose Kubewarden over any other?\nAs explained above, Kubewarden provides unprecedented flexibility, thanks to the way it evaluates\nits policies. This has the massive advantage that you will never reach a point that you have a\npolicy that you would like to enforce but are restricted by the policy engine itself. However, it\nalso has some drawbacks. The primary one being complexity. Writing WASM modules is not for the\nfainthearted, as WebAssembly is not yet incredibly mature, and most developers will not be familiar\nwith it. The complexity issue can however be sidestepped as the vast majority of policies can be\nexpressed using off-the-shelf WASM modules provided by Kubewarden.\nAnother aspect that often needs to be considered in enterprise contexts, is support. Kubewarden is\nan open source project that is loosely backed by SUSE (as it was originally developer for its\nRancher offering). Thus enterprise support is only available via a SUSE Rancher Prime. Other tools\nsuch as Kyverno are not only more mature, but offer more flexible enterprise support (via\nIsovalent).\nFinally, another aspect to consider is the featureset of a policy engines. Not all policy engines\nsupport mutating requests, and are thus much more restricted in their use. However, in this category\nKubewarden offers all the features typically desired from policy engines. Some engines such as\nKyverno support more features such as synchronizing Secret objects. While this can be useful, it\nis, in my humble opinion, not a feature for a policy engine.\nOf course, there are also personal preference aspects to consider. As an example, Kubewarden and\nKyverno handle policy exceptions very differently. Kubewarden has matchers that can be defined as\npart of the policy itself, which allow to exclude some resources from being validated. Kyverno on\nthe other hand uses a separate CRD called PolicyException.\nBoth have advantages and disadvantages.\nVerdict\nKubewarden is a very interesting piece of software. Its internal architecture enables it to be\nincredibly flexible, at the cost of complexity. However, due to a smart concept of WebAssembly\nmodule re-use, that complexity is mostly under the hood, unless one wants or needs to dive deep. In\nmy opinion, Kubewarden can be an absolutely great consideration when ones operates very large\nKubernetes clusters what might have quite exceptional requirements. However, even in these cases, I\nwould recommend starting very slow, and slowly building up to the complexity Kubewarden can hold in\nstore.\nIf you do not operate a large Kubernetes fleet, or expect to have rather standard requirements in\nterms of how you want to restrict access to you cluster(s), you might be better off with more mature\nand simpler tools like Kyverno. Getting support for these tools is likely to also be much simpler.\nA large part of the complexity of Kubewarden also comes from all that is required to even run this\nin an enterprise context. Unless you allow pulling WASM modules directly from the internet, you will\nalso need a registry to host OCI packaged modules. On top of that, should you decide to write your\nown modules, you will need a process to do this, and build knowhow in that area. These are some of\nthe aspects I hope to cover in a follow up post."},{"url":"https://f4z3r.github.io/the-tortoise-and-the-hare-do-ai-agents-really-help/","title":"The Tortoise and the Hare: do AI Agents Really Help for Software Development?","body":"Making my development workflow as fast as possible is a big passion of mine. From customizing my\ndevelopment setup to get the last inkling of efficiency out of it, to thinking how to manage notes\nand knowledge resources to access them as quickly as possible. With the sudden ubiquity of AI in\ndevelopment tools, I came to wonder how AI could help me write code faster. Being quite the skeptic\nwhen it comes to AI actually generating code for me (using tools such as Cursor or GitHub Copilot),\nI came to investigate AI agents which specialise in code reviews. In this blog post I will share my\nexperience using such an agent on a real world case. I will explore where such agents shine and\nwhere they are severely lacking.\nI am an AI Skeptic\nGenerally I am not fond of using AI to develop software. My background is mostly in systems\nsoftware, where correctness of the software can be critical. This means that using tooling that is\nnon-deterministic and might not produce adequate results makes me uneasy. Furthermore, even if AI\nwere to produce amazing results, a developer relying on it could quickly lose understanding of the\ncode. This results in skill atrophy and large risks if the AI reaches the limits of its\ncapabilities. In other words, I am not keen on having any AI generating code for me on a large scale\nfor anything more than a proof of concept or low risk project.\nNonetheless, one would be foolish to ignore AI's capabilities when it comes to developer tooling.\nAI Support Agents\nThus starts my journey investigating AI agents that can support me in the software development\nlifecycle, but whose main use is not to generate code. Many such agents exist, mostly focusing on\nreviewing code. I am quite the fan of such a use case, as the AI essentially plays the role of\nanother developer I might work with. It reviews my code, provides feedback, suggestions, and\npotentially even improvements. It however does this immediately after I have opened a pull request,\nrather than having to wait for days or weeks on a human review.\nHow is this different from using an AI that generates code you might ask? The main difference lies\nin the fact that I still have to think on how to solve the problem I am working on, and provide a\nbase solution. This forces me to understand the issue at hand. Thus, I am much better prepared to\naccept or reject any suggestions from an AI than if the AI just generated a first solution for me.\nMoreover, people (myself included) tend to be slightly defensive about the code they write. Thus I\nwill, in all likelihood, only accept AI generated code improvements if it offers a real improvement,\nrather than blindly incorporating them into the codebase.\nAll in all, it is extremely unlikely that I will lose understanding of the codebase or have my\nproblem solving skills atrophy, but I can iterate on reviews much faster.\nCodeRabbitAI\nIn order to gain first experiences with such an AI agent, I chose to try out\nCodeRabbitAI. This was not a thoroughly researched decision. The main\nreason I chose CodeRabbitAI is that I could try it out for free during 14 days and that it\nintegrates well with GitHub. I am aware that performance between AI models varies greatly. However,\nCodeRabbitAI uses Claude under the hood, a model typically known to perform surprising well on\nprogramming tasks. I thus expect it to not perform significantly worse than any other state of the\nart model out there.\nStarting Small\nIn my opinion, such agents need to be tested on real world examples. One can see demos using AI to\ngenerate a dummy web app all over the place. However, common software projects are significantly\nlarger, contain more complex logic, and are less standardized than these demos. Unfortunately, most\nsoftware I work on professionally is not publicly available, so I cannot use CodeRabbitAI on these.\nI therefore picked two (still very small) personal projects of mine:\nA NeoVim plugin providing a colour scheme.\nA command execution engine to run templated commands.\nBoth projects are extremely small, with under two thousand lines of code. Both projects are written\nin Lua, a quite uncommon language. I wanted to see how the AI fares against something it is unlikely\nto have seen too much during its training.\nWith that in mind, I wrote a\nfirst pull request implementing a fix in\nhighlight groups for pop-up menus in NeoVim. I enabled CodeRabbitAI to summarize the PR for me.\nThe summary looks good, even though it somehow marks some fixes as features. This is especially\nintriguing as I use conventional commits and\nexplicitly marked these changes as fixes. Additionally, CodeRabbitAI offers a \"walkthrough\" of the\nchanges made in the PR. In the case of such a simple PR, I found the walkthrough to be mostly\nconfusing. In the case of larger PRs I can however see how this may be appealing.\nA walkthrough of the changes in the first PR\nIn reality, I initially opened the PR with only the fixes the pop-up menus. I then pushed commits\nintroducing the support for additional plugins later on. I would have expected CodeRabbitAI to\ncomplain that the new commits introduce changes unrelated to the PR, which is not seen as best\npractice. It did nothing of the sort.\nWhile the summary, walkthrough, and disregard for best practices were unsatisfying, one unexpected\nbenefit emerged: the integration of linting feedback directly within the pull request comments. It\nprovided nitpicks from linting tools (in this case\nmarkdownlint. On one side, it is very disappointing\nto see that the AI agent did nothing more than lint the code and generate a nice comment out of the\noutput. On the other hand it is quite nice that it introduces \"quality gates\" such as linting\nwithout me having to write a pipeline for it. Moreover, producing easily digestible output from a\nlinter is nothing to be underestimated. The quality of life of having this directly as a comment\nrather than having to go through pipeline logs to read the raw linter output is quite nice. Is it\nworth two dozen USD per month? No, definitely not!\nOn the upside, it did update the summary of the PR to reflect the other changes:\nThe first PR was extremely trivial. It did not introduce any code containing logic. Other than not\npointing out that it should probably have been two separate PRs, CodeRabbitAI fared as I would have\nexpected another developer to have reviewed the PR. With two small differences:\nThe CodeRabbitAI review was close to immediate (took around 30-60 seconds to run). This is\namazing to iterate quickly.\nWhere I would have expected a human reviewer to point our the nitpick or simply approve,\nCodeRabbitAI is extremely verbose with explanations, walkthroughs, and so on. This in turn\nwastes time for the author, as he/she would need to read through this. The verbosity could be\nnicer on larger PRs, but for small concise PRs this is massive overkill and borderline annoying.\nTo further evaluate CodeRabbitAI's capabilities, I decided to test it on a pull request with more\nsubstantial changes...\nA More Complex PR\nArmed with dampened expectations from my first PR, I opened\nanother PR in the command execution repository implementing\na feature affecting multiple files. These changes also update existing logic.\nIn this second PR, CodeRabbitAI went above and beyond, and generated a walkthrough containing two\nsequence diagrams showcasing the control flow of the code that was modified! I was actually quite\nimpressed by this. While probably not necessary for the author of a PR, this is great even only for\ndocumentation purposes. New team members with less experience may benefit from such visual aids to\nunderstand complex logic within the code. Unfortunately the diagrams didn't highlight the specific\nmodifications introduced by the pull request.\nHowever, the supporting text suddenly becomes more relevant when considering such PRs.\nOne of the sequence diagrams generated by CodeRabbitAI\nOn top of that, CodeRabbitAI actually posted interesting comments. It found the odd nitpick here and\nthere, but also found more meaningful potential issues. For instance, I modified a test\nconfiguration to use a different shell. CodeRabbitAI identified that this shell is not listed as a\ndependency anywhere in the repository, and that it would thus not work off-the-shelf. In this case\nthis was only a test file used to parse the configuration and the configured shell did not affect\nanything, but this is a great finding generally.\nI also started conversing with CodeRabbitAI about some changes. Requesting it to give me a\nsuggestion on some configurations. It managed just fine, but did not actually provide these as code\nsuggestions that can be applied, but rather as code blocks in comments, which was a bit\ndisappointing.\nAdditionally, I decided to try to use CodeRabbitAI's commands feature. This enables ChatOps to\ncontrol actions taken by CodeRabbitAI. I generated the PR title using one such command. The title\nturned out generic and not very informative. In CodeRabbitAI's defense, I am quite unsure how I\nwould have named that PR.\nI then tried to get it to write docstrings for new functions that were introduced in the PR. It\nmassively misunderstood the request, and created\na PR adding docstrings to all functions in the affected\nfiles, even ones that already had docstrings... This goes to show that in some cases, it cannot even\ndo what the most junior of all engineers would be capable of doing thanks to a even so tiny dose of\ncommon sense. Moreover, it started adding commits with emojis in the title. This goes to show that\nthese AIs are probably not trained much on professional projects.\nCodeRabbitAI not only breaking conventional commits but introducing emojis...\nAfter that first disaster, with significantly less ambition, I requested it creates a PR to change a\nsmall typo. CodeRabbitAI informed me that it created a branch with the changes included, but that it\nwas not capable of creating pull requests. This shocked me, considering it had created its first\ndisaster PR no 10 minutes before.\nAfter another nudge, CodeRabbitAI however did create a PR.\nIt targeted main instead of the branch I was initially using. I guess this is my own fault though\nfor not being specific enough.\nFinally, I also tried to get it to update the wording on a commit it did to use conventional\ncommits. Unfortunately it seems that it only has access to the GitHub API and cannot execute any\nlocal git commands. It is therefore not able to perform some relatively common operations in the\nSDLC that are not part of the GitHub API. However, I am guessing this is subject to change\nrelatively soon with the emergence of technologies such as the\nmodel context protocol, which would enable it to\ncontrol external tools such as git.\nAll in all, I would say CodeRabbitAI did as I would have expected after the first PR. It corrected\nnitpicks and allowed me to perform some simple actions. Did it deliver a review of the same quality\nlike a senior engineer familiar with the project would have? No. In fact, in order to test this I\nintentionally implemented a feature that was already present in the repository, while making a\ncouple design decisions that go against most of what the rest of the repository does. CodeRabbitAI\nneither detected that the logic I was introducing was already present in the codebase, nor did it\ncomplain about the sub-optimal design decisions. This goes to show that such agents are still not\ncapable replacing humans with nuanced understanding of the project's history and architectural\nprinciples, potentially leading to the introduction of redundant or suboptimal solutions.\nDashboards!\nAnother feature of AI agents next to the reviews is the analytics capabilities that come with them.\nIn my personal opinion, analytics are important to measure the impact the introduction of such\ntooling has on the software delivery. CodeRabbitAI provides a couple nice dashboards on how much it\nis being used, and what kind of errors it helped uncover.\nActivity dashboard showing engagment with CodeRabbitAI\nDashboard showing overall adoption of CodeRabbitAI on the projects\nFindings dashboard showing errors and suggestions by type\nI did not try out CodeRabbitAI for long enough to have any meaningful metrics, but I am confident\nthat the capabilities provided are enough to get a decent understanding of the quality of adoption.\nMoreover, CodeRabbitAI supports reporting. This allows to generate reports based on natural language\nprompts that could be useful for product owners to get insights of changes made to the software over\nthe course of a sprint.\nVerdict\nWhile this whole article might seem like a slight rant against such tools, I would in fact wish I\ncould use such tools at work. Not as a replacement for human reviewers, but as an addition to them.\nFor instance, the quite verbose walkthroughs CodeRabbitAI provides can be a very helpful entrypoint\nto a human reviewer on larger PRs. Moreover, while the quality of the review is insufficient for\nprojects where quality matters, having near instant feedback is amazing.\nFinally, as mentioned above, I believe one major selling point of such agents is in the way we\nhumans interact with them. Even if the agent might do little more than execute linters or similar in\nthe background, having the output of these tools in natural language directly as comments in the PRs\nis not to be underestimated. This is especially true in the age where more and more responsibility\nis being shifted to developers. With DevSecOps, developers have to understand and act upon the\noutput of all kinds of tools. Presenting this output in a more understandable format, potentially\nenriched with explanations, can have a significant impact.\nTherefore, as a final word, I would actually encourage people to explore such agents to augment\ntheir workflow safely, albeit with caution and a clear understanding of their limitations."},{"url":"https://f4z3r.github.io/a-comprehensive-guide-to-managing-large-scale-infrastructure-with-gitops/","title":"A Comprehensive Guide to Managing Large Scale Infrastructure with GitOps","body":"GitOps is getting adopted more and more. However, there still seems to be some confusion as to what\nGitOps is, how it differs from regular CI/CD pipelines, and how to best adopt it. In this post we\nwill quickly cover what GitOps is, and the three main lessons learned from using GitOps to manage\ninfrastructure at scale both on premise and in the cloud.\nGitOps Overview\nGitOps is a set of principles enabling the operation of a system via version controlled, declarative\nconfiguration. More specifically, the OpenGitOps project defines four\nprinciples which define whether a system or set of systems is managed via GitOps:\nDeclarative: A system managed by GitOps must have its desired state expressed declaratively.\nVersioned and Immutable: Desired state is stored in a way that enforces immutability, versioning\nand retains a complete version history.\nPulled Automatically: Software agents automatically pull the desired state declarations from the\nsource.\nContinuously Reconciled: Software agents continuously observe actual system state and attempt to\napply the desired state.\nNote that git is not referenced anywhere, as GitOps is not bound to any tooling. However, in\nlayman terms, many consider a system operated via git to be a GitOps system. This is not quite\ncorrect.\nGitOps is More than CI/CD Pipelines\nTaking the \"layman's definition\" from above, any system that has CI/CD via pipelines triggered on\nrepository changes would be a GitOps system. This is not accurate. Consider an IaC pipeline which\napplies declaratively defined infrastructure (such as a standard opentofu apply in a pipeline, or\na Docker build followed by a kubectl apply). While such a system adheres to the first two\nprinciples, it does not adhere to the latter two. This implies that changes made to the target\nsystem are not corrected (reconciled) until the pipeline runs the next time. Similarly, if the\npipeline fails for whatever reason, the desired state does not change the pipeline: a configuration\ndrift is not detected, even if not reconciled.\nThis is an important distinction when considering \"standard CI/CD\" and GitOps. Simply having\nsomething declared as code does not make it GitOps.\nThe Advantages of GitOps\nGitOps has many advantages over standard ways of managing systems. The advantages of having a\ndeclarative desired state, version controlling it, and interacting with the system only via git\n(or whatever version control system you use) are tremendous. From improved security and higher\nefficiency to better change visibility. These are well known to most people and will thus not be\ncovered here.\nDrift detection and automatic reconciliation are the two other aspects that make GitOps absolutely\namazing. This is especially true in the current day and age, with the proliferation of complex\nsystems being worked on by many people concurrently. Being able to observe that the system is not in\nthe desired state has massive advantages, such as for standard SRE operations. Continuous\nreconciliation ensures that manual operational tasks are kept to a minimum, and that systems cannot\ndegrade over time as small undesired changes creep in.\nTooling\nIn this post we will mostly focus on using GitOps to manage resources handled via the Kubernetes\nAPI, but it should be noted that GitOps as a concept is in no way restricted to Kubernetes. In the\nKubernetes space there are two major players for GitOps: ArgoCD\nand FluxCD. We will not go into the details as to what the advantages for each\ntool are, other than saying that according to our own experience, ArgoCD might be more developer\nfocused, while FluxCD might suit platform engineers with more Kubernetes experience that want more\nflexibility.\nThe rest of this post is tool agnostic and everything we are talking about can be done with either\ntool (but some aspects might be easier to do with one or the other).\nInfrastructure: Disambiguation\nBefore we dive into how to structure your GitOps configuration, it might make sense to draw a line\nas to where infrastructure starts and where it ends. We consider infrastructure everything that is\npart of the platform provided to an application team. Hence this line might vary depending on the\nmaturity of the platform you provide your teams. If we consider a simple Kubernetes platform with\nlittle additional abstraction for its users, the infrastructure would contain the Kubernetes\nplatform itself as well as all system components that are shared between the teams, such as a\ncentral monitoring stack, a central credential management solution, centralized policy enforcement\nof specific Kubernetes resources, and the like.\nThe lower end of the spectrum will likely not be managed by GitOps. That is simply because the\nGitOps tooling itself typically needs to run somewhere, and also needs to be bootstrapped somehow.\nSome tools such as FluxCD allow the GitOps controller to manage itself, but even in these cases the\nruntime for the controller needs to exist when the controller is initially installed, and is thus\ntypically not part of the GitOps configuration.\nNow that this is cleared up, let us consider how the configuration should be managed.\nApp-of-Apps\nA very popular pattern for managing configuration via GitOps is the \"app-of-apps\" pattern. This was\npopularized by ArgoCD, but is also applicable to other tooling. We will use ArgoCD in the example\nbelow, but the same can be implemented using FluxCD Kustomizations.\nLet us consider a component from our infrastructure that we want to manage via GitOps. Typically, we\nwould need to tell the GitOps controller how to manage this component. For instance, let us assume\nthe component is installed via raw Kubernetes manifests. Then we would tell the GitOps controller\nwhich repository contains these manifests and in which namespace to install them. Depending on the\ncontroller you are using, you might also configure additional parameters such as how often it should\nbe reconciled, whether it depends on other components, and so on. In ArgoCD jargon this would be an\n\"Application\" (the root of \"app-of-apps\" naming), and would look as follows:\nYou would then apply this Application resource to Kubernetes. Your component would then be managed\nby GitOps, as any changes you push to the manifests repository would be reflected on the Kubernetes\ncluster.\nThen a second infrastructure component needs to be installed, and you repeat the process. The result\nwould be a second Application which installs and manages a component. You might also want to\nversion your deployment (such as using version 1.16.1 of the Helm chart). This implies that\nlifecycles require a change to this Application manifest, and thus a call against the Kubernetes\nAPI to edit it.\nThe end result is a set of Application resources, some of which you periodically modify when\nlifecycling a component. Now imagine you need to deploy your infrastructure elsewhere (for instance\na second Kubernetes cluster in our example), or maybe even a couple dozen times. Then you need to\nmanage this entire set of Application resources on every platform. A better approach is to add an\nabstraction layer, which itself deploys the Application resources via GitOps. Hence you put all\nyour Application resources into a repository, and define another, \"higher level\" Application\nwhich deploys this repository. This means that when deploying to new platforms, you only need to\ndeploy that one \"higher level\" Application, and any changes to the component Application\nresources can be made via Git, conforming to our GitOps approach. This \"higher level\" Application\nis only there to deploy the component Applications thus the name \"app-of-apps\". Visually, you thus\nhave the following structure:\nIt should be noted that this also massively helps when customizing platforms. Typically, components\ncannot be deployed truly one-to-one in several places, but require slight configuration differences.\nConsider for instance hostnames for UIs of your components. Two of these components deployed in\ndifferent locations cannot share the same hostname and routing. Using an \"app-of-apps\" approach\nallows you to define variables on the top level application, and inject these into the downstream\napplications such that they can slightly adapt the way they are installed. We will not dive deeper\ninto how this is done as it is highly dependent on the tooling you use (ArgoCD uses\nApplicationSet, FluxCD uses variable substitution), but know this is enabled by such an approach.\nConsolidating your Configuration\nIn the organisation I first used GitOps at scale, we deployed all our components as Helm charts to a\nKubernetes cluster. Each component was essentially contained within two different repositories in\nour version control system:\nthe source code repository which typically built a Docker image as an artefact\nthe Helm chart definition which referenced the Docker image from above\nWhen we then introduced GitOps, we decided to add a third repository containing the exact deployment\ndefinition (in our case the Application declarations) for the component. Using the app-of-apps\npattern from above, we could then reference each of these \"GitOps repositories\" and deploy specific\noverlays (customizations) of the Application to specific platforms. This worked well for quite\nsome time. However, with time the number of components we managed increased, and so did the number\nof target platforms to which these components needed to be deployed. This lead to quite a few\nissues.\nWhen a new target platform was introduced, all such \"GitOps repositories\" needed to be updated to\ncontain a new overlay customizing the Application to the specific platform. This is very tedious\nwhen you have several dozen such repositories.\nMoreover, components had dependencies to other components. This meant that we were referencing\ncomponents within a repository that were defined in another repository. While not problematic in\nitself, this can become very tricky when one component has a dependency on a configuration value of\nanother component. The configuration value is then duplicated in both repositories and becomes\ndifficult to maintain. While this sounds like we did not properly separate the components, it is\nvery common to see such cases in infrastructure configurations. Consider for instance a deployment\nof an ingress controller which defines a hostname suffix for its routes. All components deployed on\nthe same Kubernetes platform that deploy a route/ingress will need to use exactly that hostname\nsuffix in order to have valid routing.\nThe above issue also results in tricky situations when configurations need to be changed for\ncomponents that are dependent on one another. If the deployment configuration is separated into\ndifferent repositories, PRs to these repositories need to be synchronized to ensure the deployment\noccurs at the same time.\nFinally, distributing the deployment configuration over so many repositories meant that it became\nincreasingly difficult to have an overview of what is deployed on a target platform. One would need\nto navigate through dozens of repositories to check this is correctly done.\nAfter identifying these issues we decided to move all our configuration into a single repository.\nThis repository would then contain a templated definition of the entire set of components which\nwould need to be deployed. A set of platform definitions within the same repository would then feed\nvalues to templates to ensure consistent configuration. This massively helped us with to address the\nissues mentioned above. On top of that, it allows to version the \"template\" and thus enables\nrollouts of a versioned infrastructure layer. You can find an example repository of such a structure\ndesigned with FluxCD here: FluxCD Monorepo Demo.\nGitops Bridge\nThe last challenge we want to address in this blog post is a concept called a \"GitOps bridge\". In\npublic cloud environments, there is typically a relatively strong cut between infrastructure\ndeployed via Terraform (or any similar tool), and the infrastructure deployed via GitOps. For\ninstance, one might deploy an Azure Kubernetes Service and some surrounding services (such as the\nrequired network, a container registry, etc) via Terraform, and them deploy components and\napplications within the AKS using GitOps. The issue that we face here is that the GitOps\nconfiguration very often depends on the Terraform configuration. Consider for instance the container\nregistry. Its address is set up by Terraform, but is used in every image declaration in the GitOps\nconfiguration. One option is to duplicate such values in the respective configurations, while\nanother option is to use a GitOps bridge.\nThe GitOps bridge is an abstract concept on how to pass configuration values from tooling such as\nTerraform as inputs to the GitOps configuration. How this is done in practice very much depends on\nwhich tools you use. For instance, if looking at Terraform and FluxCD, a common way to achieve this\nis to have Terraform write a ConfigMap onto the AKS where the FluxCD controller will run containing\nall variables (and their values) that will be required by the GitOps configuration. The FluxCD\ncontroller then supports injecting variables from a ConfigMap via\nvariable substitution.\nUsing a GitOps bridge has the advantage that changes in the Terraform configurations are much less\nlikely to break the GitOps configuration that builds on top of it. Moreover, it allows Terraform to\ndirectly bootstrap the entire GitOps setup when creating new platforms without the need to manually\nredefine the required variables in the GitOps repository.\nSummary\nSo, to recap, we have looked at what GitOps really is (and isn't). Understanding these basics is\ncritical to correctly implement GitOps in your projects. On top of that, we looked at three best\npractices:\nUse an app-of-apps pattern to improve resiliency for when you need to recreate platforms.\nConsider using a mono-repository for all your GitOps configuration as your setup grows.\nHave a look at GitOps bridges to improve the automation when setting up platforms and ensuring\nyour Terraform and GitOps configurations are consistent.\nI hope this has helped you understand a bit better how to use GitOps at scale. If you have any\nquestions or comments, feel free to let me know below."},{"url":"https://f4z3r.github.io/a-very-deep-dive-into-docker-builds/","title":"A Very Deep Dive Into Docker Builds","body":"Containers are everywhere. From Kubernetes for orchestrating deployments and simplifing operations\nto Dev Containers for flexible yet reproducible development environments. Yet, while they are\nubiquitous, images are often built sub-optimally. In this post we will be looking at a full example\nof a Docker build for a Python application and what best practices to consider.\nThis is a real world example from an very small component we built in Python for one of our\nclients. Very few alterations were made to the original configuration (changing URLs, and removing\nEmail addresses mostly). We will go in depth as to why we did every single little thing. While\nsome stuff is quite Python-centric, the same principles apply to other languages, and the text\nshould be broad enough so that it is understandable how to transfer this example to different\nlanguages.\nAlso, this is a long article, so if you actually plan on reading it, grab yourself a snack and a\nfresh drink first.\nGoal\nThe goal of this post is to showcase how one can setup a Docker build that is:\nfully reproducible,\nas fast as possible,\nfails early on code issues,\nisolates testing from deployed code,\nis secure.\nThe example we will use for this implements quite a lot to ensure only quality code reaches\nproduction, and that it can do so as fast as possible. Going all the way might not be necessary for\nall projects using Docker. For instance, if you release code to production only once a day (or less)\nyou might care less about release build cache optimization. This example is however meant to show\nthe \"extreme\" to which you can push Docker, so that you can (in theory) push code to production\nfully continuously (CI/CD principles). But yeah ...\nWhy\nWhy do we have these goals? Reproducible builds are one of the most important factors for proper\ncompliance, and for easier debugging. Debugging is simpler, since we ensure that no matter the\nenvironment, date, or location of the build, if it succeeds, the same input generates the same\noutput. Moreover, it brings stability, as a pipeline might not suddenly fail on a nightly build (if\nyou still do such things) because a new upstream library or program was released that is used\nsomewhere in your supply chain.\nRegarding compliance, we need to be able to tell and revert to the exact state of software that was\ndeployed in the past. Without reproducible builds, using Git to track back to a previous state of\ndeployed code does not help you much, because while you can validate what code you deployed, you\ndon't know what versions of everything else you deployed with it.\nBuilds should be fast, and fail fast. The reason here is that no one likes to wait. You don't want\nto wait for 2 hours to figure out whether a tiny code change breaks tests or does not even compile.\nYou will want to isolate test code from deployed code, because more code equals more bugs. While\ntesting frameworks are very good at isolating test code from code being tested, writing tests\ngenerates a risk of bugs. Moreover, the test code is unneeded bloat for your runtime application.\nThus it should be isolated from it.\nFinally security. While some people think that containers improve security by default, this is not\nthe case. Container technology has the potential to indeed improve the robustness of some security\nmeasures and controls. However, in order to achieve this, one needs to correctly utilize containers\nand build the images with security in mind. For instance, if an image contains certain utilities\nthat allow it to connect to the internet (such as curl or wget), it suddenly makes the container\nmuch more vulnerable to container escape attacks (where an attacker manages to move from the\ncontainer to the underlying host), and hence the whole isolation benefit of the container (which can\nbe a security control) is broken. The same is true for containers that contain interpreters and\nallow the runtime user to open, edit and execute arbitrary files. As our container will contain\nPython code, and hence the Python interpreter, this is definitely something we need to take very\nseriously.\nPython Goals\nOur example is based on Python, an interpreted language. This is not ideal, as it means that it does\nnot require a compile step. Compilation optimization is however a very important aspect in Docker\nbuilds. In order to still address this, I will talk about this, but will not refer to the\nconfiguration examples. One could ask why I did not take a compiled language example then. The\nreason is very simple, I wanted a real world example such that this post is not just theoretical\ngoodness, and most Golang image builds I am currently working on are more basic and not as\neducational.\nYet another question could be \"why deploy Python in Docker in the first place?\". This is a very\nlegitimate question. Python requires a lot of OS bloat to just be able to run. This means that\ntypically a VM is a good choice to host it. For all those saying that Docker is still better because\nof performance (due to faster startup, no hardware virtualization overhead, etc): this is not true\nfor such cases where a large part of an OS needs to be in the Docker image. A VM of a full\ninit-based Linux system can be launched in less than 250ms on modern technology. A full Ubuntu\ninstallation with systemd can be completely booted in around 2.5 seconds. The former is in the same\norder of magnitude that it might take the Python interpreter to just load the code of a large Python\napplication.\nSo performance cannot be said to be better with Docker, why choose Docker then? Better reasons are\nthat you can strip down a Docker image much easier than an OS. This is critical for us due to\nsecurity requirements. While Python requires a lot of OS features, the majority of the OS is still\nbloat. Every piece of bloat is a potential attack vector (each of these unused components might have\none or more CVEs that we need to patch, even though we don't even use that software). Another reason\nis that the build process of Docker is much simpler to manage. There are tools such as\nPacker that allow similar processes for VMs, but these are not as\nstandardized as the open container initiative (OCI - which Docker\nadheres to).\nAnother very important point is the ease of development. Docker and other OCI compliant products\nprovide us with a possibility to build, test, and run our build artefacts (in this case Docker\nimages) everywhere. This makes it very simple and fast for our developers to test the build and\nperform a test run of an image locally on their development machine. This would not be quite the\ncase with VMs or raw artefacts (JARs, source code archives, ...). Moreover, the OCI ecosystem does\nnot only include specifications on how to interact with images, but also how to setup and configure\ncritical elements such as persistence and networking. These aspects are made very simple with\nDocker, and would be quite a pain to manage securely with most other technologies.\nFinally the main reason for us is the choice of runtime. We have very decent container runtimes\n(RKE,\nRHOS, K3s) available\nto deploy applications. We are very familiar with them, and they offer us a lot of functionality.\nThese all support containers primarily.\nA Tiny Bit of Background\nLast before we get into the dirty details, a tiny bit of background into what we are building. The\napplication we will be building here is a sort of a facade reverse proxy. It offers a standardized\nAPI to clients, which can connect and perform requests. Based on the content of the request, the\ncomponent will trigger a routing algorithm that defines where the request needs to be routed. This\nrouting algorithm might require several API calls in the backend to different systems to figure out\nwhere the call should go. Once done, the component will relay the call to a backend, and forward the\nresponse to the client. The client is never aware that it is talking to more than one component, and\nonly needs to authenticate to that single system. Imagine an API Gateway, but where the routing is\nextremely complex and requires integration with systems such as Kubernetes, a cloud portal, and\nmore.\nThe Details\nHere is an overview of our Dockerfile:\nWe will go through it line by line and figure out why we did what we did, and why we did not choose\na different approach. Let's start!\nIn this line we reference a container image for later use and provide it a name alias cert-bundle.\nThis container image contains only data: our production network proxy certificates and all internal\ncertificate authorities. We need these CAs as we will connect over TLS to backend components that\nhave internal certificates. We also need the production network proxy certificates as we will pull\ndependencies straight from the internet, and all that traffic is routed over a gateway proxy. Why\ndistribute these certificates over a Docker image instead of a compressed TAR? The main reason is\nthat we want to have a unified way that we build artefacts and manage CI/CD pipelines. By creating\nand managing the certificates via Docker, we can use our entire Docker setup (such as\nUCD/Jenkins/Tekton pipelines for building, registry for distribution, quality gates for security,\netc) and do not need to have a different system to manage the certificates. Note that we refer to\nthe exact state of the certificate bundle (20220405), which refers to the state of the\ncertificates per 5th of April 2022. This is very important to make the build reproducible. If we did\nnot pin the version of the certificates, it would mean that we could build the image maybe today,\nbut it would fail tomorrow, once the certificates change (even though we did not change the code at\nall). You will note that we will pin every single version in the entire build process.\nIn this line, we reference the base image we will start building from. This is the official Python\nimage for Python version 3.9.2. We use the slim version because we don't need much more than the\nstandard Python installation. We pull this from our own registry, as all Docker images are scanned\nbeforehand to reduce the risk of supply chain attacks. Also here, the version is pinned. We provide\nthis build step the builder alias. In essence this means that starting from this line we define an\nimage stage that will contain the build process of our application. For Python, this mostly includes\ndownloading dependencies (both software and system level), and injecting the source code, as there\nwill be no compile step.\nThis copies our certificates into our build image. We do this by referencing the build step\ncert-bundle (see first line of the Dockerfile again) in the --from argument of the COPY\ncommand. Note that we could have referenced the image directly in the --from argument. We choose\nto use build stage aliases for visibility, and reduce duplication if the certificates need to be\ncopied into different stages. Note that this copies only the raw certificates. A OS specific bundle\nwould still need to be generated.\nHere we do exactly this, we generate a certificate bundle for the underlying OS of our builder image\n(Debian). This allows our subsequent build steps to use the certificate\nbundle to validate host certificates on TLS connections.\nWe then set a working directory. The idea is to have a base directory on which we now operate. This\ncan be nearly any working directory, and will be created if non-existent. We choose /app/ by\nconvention. Moreover, note that we tend to reference directories with the trailing / to make it\nmore explicit that we are referencing directories and not files. We use this convention throughout\nthe configuration/code.\nWe use an environment virtualization technology for Python. This is called\npipenv. It allows us to have many different\nversions of the same dependency installed locally, without them conflicting. This is very important\nwhen you are developing many applications at the same time locally. By running this line we install\nversion 2024.2.0 of pipenv (pinned). Other than Python itself, these are the only tools required\nfor our Python development environment. If we were using a different language, pipenv would be\nsubstituted with your dependency management tool (such as Maven for Java). Note that we only install\npipenv itself, we do not install the dependencies. Also using the flags provided we ensure a fully\nclean install of pipenv.\nThis is an example where we reach out to the internet and thus needed the network proxy\ncertificates.\nA very good question here might be \"why to use pipenv at all, considering it is typically used for\nenvironment virtualization, which is already covered by Docker itself?\". There are two aspects here.\nThe first is to allow us to lock dependencies using their hash, which is not natively supported by\npip (the standard Python package manager). The second is that we want to keep the build process\nwithin Docker as close to the build process outside of it. While we do not build artefacts outside\nof Docker per-se, the IDEs of our developers need to fall back on these technologies to support\nfeatures such as library-aware code completion, type-checking, test integration, debugging, etc.\nThis could also be achieved by connecting the IDE to an instance running directly in Docker. This\nhowever is relatively complex and requires the setup to support remote debugging. In theory, these\nare not really problems as long as the dev environments are uniform, but we allow each developer to\nwork with the tools he/she desires to develop code. It then suddenly becomes very difficult to have\na stable setup that works for everybody, especially considering that some of our developers do not\nwant/know how to configure their environments to that level (client-server debugger setups, network\nand volume management between the IDE and Docker, ...).\nHere we set environment variables for pipenv. Firstly we want the dependencies to be installed\ndirectly in the project repository, not centrally. This allows us to ensure that we do not\naccidentally copy a system Python dependency that installed by default with the base image. The\nsecond configures the certificate bundle we generated in the beginning to be used by pipenv. It\ndoes not use the system configured bundle by default, so it needs to be configured manually here.\nNow the interesting stuff. Here we copy the dependency files into the image. The first file contains\na list of dependencies that we use for our project. The second contains a hash the dependencies\nshould have, including indirect dependencies (dependencies of dependencies), in order to ensure that\nwe always get exactly the same dependency code for very install. The first looks as follows:\nNote that we split dependencies into normal packages we require for our application, and packages\nonly required for testing and our quality gates ([dev-packages]). This is important later on, as\nwe do not wish to have packages only required for testing in our production Docker image.\nI will not show you an example of the lock file, as it contains mostly checksum hashes. Simply trust\nme that it contains the exact checksum that every package (such as the dependencies of requests)\nhas to have to be installed. The reason this is required in the first place, is because the\ndependencies of requests are likely not pinned to an exact version and might thus change between\ninstallations unless locked via our Pipfile.lock. This would undesired as it would make our builds\nun-reproducible. The lock file itself is generated by our developers in two different scenarios. The\nfirst is when a library is added due to some new feature. In such a case the new library is added to\nthe Pipfile, and an installation is triggered outside of Docker. This will install the new library\nand potentially update already installed ones (in case of conflicts). Hence new hashes will be added\nto the lock file. The second is on a lifecycle of the existing libraries or of our Python version.\nIn such a case we update the pinned version in the Pipfile and trigger an installation outside of\nDocker. Again, pipenv would then update the direct dependencies, and potentially transitive ones,\nand update their hashes in the lock file.\nHere we install the dependencies for our application. The --deploy flag means that we want to\ninstall the dependencies based on the lock file. Moreover, we do not install the dev packages yet,\nonly the ones needed for the production code.\nHere we generate a new Docker build stage. We have generated a stage with builder that contains\nthe required certificates and the production dependencies, and nothing more. We now want to test our\ncode and validate quality gates. We do not want to perform this in the builder stage, because it\nwould pollute our production dependencies. Moreover, using a different stage allows to trigger\nbuilds more granularly with BuildKit. For instance, I\nwould be able to configure (with --target=test) to only build the image up to the test stage,\nand skip any later stages (such as the runtime image in our case). This can be very useful in\npipelines, for instance, where we want to run the test on every commit, but are not interested in\nbuilding a real artefact unless the commit is tagged.\nWith this line we essentially say \"start a new stage called test from the latest state of\nbuilder\". We also add a comment above to make it more visible that we are starting a new stage in\nthe Dockerfile. Stage comments are typically the only comments we have in the Dockerfiles.\nIn this line we now deploy the development dependencies, including tools for quality checks (mypy,\nbandit, black, see below for details) and for testing. Again, we use the --deploy flag to\nensure we always use the same versions to make the build fully reproducible.\nHere is the first time we copy actual content, other than the list of dependencies, into our image.\nThis means that up until now all layers in the build process can be fully cached if we perform code\nchanges. Thinking about this is primordial if you want an efficient build process. Even here, we\ncopy code in reverse order based on likelihood of change. The first file we copy in configures our\ntooling and quality gates. This is unlikely to change unless we introduce a new tool or change\nconfiguration of an existing one. An example of the file can be seen below.\nThe second line copies assets. These are used for testing, such as test configurations for\nconfiguration validation etc. These are also quite unlikely to change unless we write new tests of\nour configuration classes.\nThe third line copies in our Cucumber files for BDD\ntesting. These change only when we either define new behavioral tests or add features.\nThe fourth line copies our test code, this is quite likely to change, as it contains all our unit\ntests, and the testing framework for behavioral tests.\nFinally the last line copies in our actual code. This, along with the unit tests, is the code that\nis most likely to change, and thus comes last. This way on a code change, all lines up to this one\n(assuming we did not add/change tests) can be used from cache.\nThis line aggregates our quality gates and testing. For quality gates we have:\nmypy: checks typing information where provided. We do not perform\nstrict typing so that type information is required everywhere, but we validate that the provided\ntyping is correct.\nblack: checks formatting of the code to ensure it is\naccording to your guidelines.\nbandit: performs basic security checks. This is a\nnon-blocking check, meaning that the build will only fail if issues of severity MEDIUM or higher\na found. LOW severity check fails are ignored.\nFinally we run our testing (with pytest). We run the testing\nlast, as it is the most time consuming of the tasks, and it does not need to be executed if the code\nfails to adhere to our standards. Note that you could add any other gates here, such as a code\ncoverage baseline that needs to be adhered to, various code analysis checks, or security scans. We\nonly perform one more security check against dubious code and supply chain attacks during the build\nprocess. This check is however done on the final Docker image and is thus executed by the pipeline\nitself outside of the Docker build process.\nNote that all commands are executed as one RUN statement. This is best practice, as none of these\ncommands can be cached individually. Either all have to be executed again if layer it builds upon\nchanged, or none has to run. Putting them into the same RUN statement generates a single new layer\nfor all four commands, which reduces the layer count and build overhead for Docker.\nFinally, note the --mount options passed to RUN (introduced with BuildKit 1.2). These allow to\ncache content within the Docker build between builds. Here we mount two caches, one for mypy and\none for pytest. These ensure that if a subsequent Docker build is triggered for code that does not\naffect some files, the typing checks and tests are not run again for these files, but taken from the\ncache. For pytest this is actually done on a \"per-test\" basis, ensuring tests are not run unless\ncode they are testing is changed. Such caches can massively increase the speed of your pipelines,\nespecially when your project grows and the test suites start to take more time to run through.\nThis defines the runner image. We are done with testing and want to build the productive artefact,\nas all checks have passed. In a compiled setup, this would mean we would now have a release\ncompilation stage (before building the runtime image). This is done after testing as the release\nbinary/JAR will be compiled with optimizations, which can take quite long, and is unnecessary if the\ntests fail anyways. Thus in a compiled language like Java or Golang, we would now continue from the\nbuilder again, copy the code back into the layer, and compile. Here one should be careful, most\nlanguages support incremental compilation to reduce compilation times. When this is supported, one\nneeds to mount a build cache, or the incremental compilations from previous builds will be lost\nevery time the code changes, as the entire compilation layer will be discarded from the cache. This\nis done the same way as in the previous block, with --mount parameters.\nOnce the compilation is completed, and we have our final artefact (binary or JAR), we want to copy\nit into the runtime image. The idea is again to restrict bloat to reduce our attack surface. For\ninstance, in a Java setup, we only need a working JRE to run our application, we no longer need\nMaven, the Java compiler, etc. Thus, after the build process, we use a new stage for the runtime\nimage. This is what we did for Python here, since we have no compilation step. We use a different\nimage than our initial internal.registry/base/python:3.9.2-slim image, as we no longer need pip\n(the Python package manager), and other bloat. Instead we use a distroless image, which is\nessentially a stripped down Debian image containing truly the base minimum to run Python code, but\nnothing to manage it, etc. Again, we use our own copy of the distroless image from our scanned\nregistry.\nThis line adds metadata to the image. This is not necessary to have a good image, but useful when\nusing images that are shared across huge organisations. This is the official maintainer label we\nuse, where we reference our team, such that anyone that downloads the image and inspects it can see\nwho built it, and how to get into contact with us in case of issues.\nSame as before, we copy certificates and configure Python to use our bundle. Note that this time we\ndirectly copy the bundle generated in the builder and not from the certificate image, as we need a\nbundle and cannot create it in this image (update-ca-certificates is not contained in the\ndistroless image). We need to copy this explicitly since we started from a fresh image. The test\nstage had the bundle implicitly configured from the builder stage, upon which it was set up.\nWe set a working directory again. This is also necessary since starting from a fresh image. Also we\nset a non root user. This is necessary since we do not want to run our code as root for security\nreasons (reduce the impact of a remote code execution - RCE vulnerability). Note that any statement\nafter the USER statement will be executed in the context of that user. Therefore I would for\ninstance not be allowed to run update-ca-certificates (if it was present in the image) in a RUN\nstatement from now on, as this requires root privileges.\nHere we copy the non-dev packages from the builder stage into our productive image. Note that we\nuse a path from within the project root (/app/), since we set pipenv to install the virtual\nenvironment directly in the project (the PIPENV_VENV_IN_PROJECT variable). We copy the\nsite-packages (the dependencies) directly into a subfolder, in which our application will live. This\nensures that they are treated as if we wrote them ourselves, as individual Python modules in our\ncode. They essentially become indistinguishable from our own code. This allows to keep consistency\nin our module names are resolved. Note we need to add the --chown flag, as the dependencies were\ninstalled by the root user in the builder image, and they need to be readable by our user 1000\nthat will run the application. The --chown flag will change the files' owner (and group) to the\nprovided argument.\nThe second line simply sets the new working directory to be the new project directory into which we\ncopied the code from the dependencies.\nHere we copy the source code back into the production image. We did this after copying the\ndependencies, such that the dependency layer can be cached again. Moreover, we only copy the source\ncode, no tests, no assets, no Cucumber features. All these latter ones are not needed to run our\napplication. Finally note that we copy it not from the test stage, but again back from the outside\nbuild context. This is because we mock a lot during testing, changing some code behavior\ndynamically. Copying it back in from the outside context ensures we do copy the exact code that is\nin our Git repository, and not something that was accidentally modified during testing, etc.\nFinally we set an entrypoint and a command. The entrypoint defines what will always be executed on a\nDocker run (unless explicitly overwritten), and the command provides the default arguments unless\noverwritten via the Docker run arguments. We always use lists instead of full strings to ensure that\nthe arguments get passed to the Kernel as system calls instead of being executed by a shell. This is\nimportant to ensure proper signal handling (when you want to terminate containers), and since there\nis simply no shell in the distroless image we are building.\nThat's it\nHoly molly... There is a lot that goes into building a simple Docker image. And that considering we\ndid not even compile anything, which would require a decent amount of extra work, and that all our\ntooling can be managed directly via pipenv and do not need to be installed separately via curl\nor some OS package manager.\nSo is it worth it? To put so much thought into how a simple Docker image gets built? I would argue\nyes. I will not start an idiomatic discussion on the benefits of smaller images, security best\npractices, or having tests being run directly in the Docker build. If you want such a discussion, go\nto Reddit or Youtube, you find plenty of beef between people fighting about these topics like their\nlife depends on it. All I will say is this:\nI can run docker build ... after each save on a file, since the caching is optimized to a point\nwhere a full build on a code changes takes about 1-2 seconds. Being able to run this so often\ngives me the confidence that what I will push will actually pass in the pipeline.\nUsing proper caching makes me avoid having to wait 2-5 minutes each time I want to compile\nsomething. Since 2-5 minutes is typically too little for a context switch to something else, it\nmight be time I would have just sat around thinking about how much it sucks to wait on stuff. So\nit has considerably improved not only my productivity, but also my mood.\nDocker avoids some \"it works on my machine\" issues. With proper version pinning and fully\nreproducible builds, it really nearly eradicates the issue. Now the only time something like this\ncan happen is when running on different Docker versions.\nWe all sometimes would like to fix tests by skipping them to \"save time\" when something needs to\ngo to production quickly. Since testing is fully baked into the build process, changing flags on\nJenkins/Tekton/whatever will not allow you to skip any testing or quality checks on the code. The\nonly way would be to comment out the test code or update the Dockerfile, which would not pass a PR\nreview. This gives me immense peace of mind.\nSince the build process and testing is (nearly) fully defined in the Dockerfile which lies in the\ngit repository, we nearly never need to change pipelines to add/change/remove anything, as all of\nthis can be done in the repository of the corresponding image directly. This also has downsides, as\nit creates duplication. I would argue that this is beneficial though, as legacy applications might\nnot be able to switch to newer tooling as fast as greenfield projects, which want to leverage that\nnew tooling. Having this \"configured\" in each repository allows each to move at its own pace. Strict\nguidelines (such as we don't want to use tool X anymore) can still be enforced on pipeline level via\ncontainer scanning tools (which you will need either way).\nWhat's the major downside of this approach? Well I would argue there is one large one. Many people\nmight not understand Docker well enough to figure out how the build process works, or might not have\ntime to invest to learn how to do it correctly. This means that some people might not be able to\nmake changes to the build processes by themselves and need might help. I think this would also be\nthe case without a proper Docker setup, but maybe this problem is augmented by having a slightly\nmore complex Docker build setup.\nI hope this has given you some food for thought. Feel free to comment any questions or remarks\nbelow, or to reach out! Do you also take your Docker builds this far?"},{"url":"https://f4z3r.github.io/pages/","title":"","body":""},{"url":"https://f4z3r.github.io/about/","title":"About Me","body":"Hi there \nI am f4z3r, a software engineer who spends 99% of his time optimising his setup to make it work 1%\nfaster. I am passionate about cloud native software and infrastructure, security tooling, and\nvarious programming languages.\nI work on a variety of projects in various roles, such as:\nas a principal software architect at ipt, working on various mandates.\nregularly contributing to open source, on projects such as\nwasmCloud,\nHashiCorp Vault, the Kubernetes ecosystem, and various\nothers.\nmaintaining of the gruvbox-material.nvim\nNeoVim plugin.\nwriting of a couple Lua tools and libraries such as sofa,\nluatext, luatables,\nand luakube.\nhacking away on my NixOS setup.\npreaching on split keyboards, NeoVim, and other pretty useless but incredibly fun things to anyone\nwhat wants to lend me their ear.\nI am currently very interested in Rust, Zig, and WASM as a runtime for serverless code on\nKubernetes. Moreover, I am trying to re-invent the wheel by building CLI libraries for LuaJIT, just\nbecause...\nI am okay at\nGPG\nI mostly use two keys:\nTo sign commits on GitHub: 358A6251E2EDEDC1971714A796A8BA6EC8712183\nTo sign emails: 43BD3E821555844CC562573E6B493A3A93726387"},{"url":"https://f4z3r.github.io/archive/","title":"Archive","body":""}]